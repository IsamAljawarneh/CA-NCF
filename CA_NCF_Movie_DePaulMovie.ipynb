{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NCF Recommender with Explict Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#Context-Aware Neural Collaborative Filtering (CA-NCF)\n",
    "#Author: Isam Mashhour Al Jawarneh\n",
    "#Date: 30 January 2019\n",
    "\n",
    "from zoo.models.recommendation import *\n",
    "from zoo.models.recommendation.utils import *\n",
    "from zoo.common.nncontext import init_nncontext\n",
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.dataset.base import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.optim import *\n",
    "from bigdl.util.common import *\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = init_nncontext(create_spark_conf().setAppName(\"CA_NCF Tripadvisor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary libraries\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas to read data from csv file\n",
    "df = pd.read_csv(\"datasets/DePaulMovie/ratings.csv\", names=['UserID', 'ItemID', 'rating','Time','Location','Companion'], header=None, usecols=[0,1,2,3,4,5])[['UserID', 'ItemID', 'rating','Time','Location','Companion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for constructing objects containing items, the contexts and context conditions\n",
    "class MyClass(object):\n",
    "    def __init__(self, item,condition,context):\n",
    "        self.number = number\n",
    "        self.item = item\n",
    "        self.condition = condition\n",
    "        self.context = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isam/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py:3506: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/isam/.local/lib/python3.5/site-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item tt2251217 cmax is 3.412806954531787 con Home context Location\n",
      "item tt3203616 cmax is 2.9541957835039856 con Weekday context Time\n"
     ]
    }
   ],
   "source": [
    "######ITEM SPLITTING Procedure############\n",
    "## as described in :\n",
    "## Experimental evaluation of context-dependent collaborative filtering using item splitting\n",
    "## https://dl.acm.org/citation.cfm?id=2579997\n",
    "#df = df1.head(1000)\n",
    "## output a list (item, condition, context), where condition is the context condition that will be used \n",
    "##for splitting the item\n",
    "\n",
    "#profiling data\n",
    "some_list = []\n",
    "contextFactors = ['Time','Location','Companion']\n",
    "dflist = df['ItemID'].unique().tolist()\n",
    "\n",
    "cmax = 0.0\n",
    "con = ''\n",
    "context = ''\n",
    "for i in dflist:\n",
    "    cmax = 0.0\n",
    "    for j in contextFactors:\n",
    "        #print(j)\n",
    "        conditionList = df[j].unique().tolist()\n",
    "        #array_length1 = len(conditionList)\n",
    "        #print(array_length1)\n",
    "        for k in conditionList:\n",
    "            #print(k)\n",
    "            item1 = df[(df['ItemID'] == i) & (df[j] == k)]\n",
    "            item1.reset_index(inplace= True)  \n",
    "            item2 = df[(df['ItemID'] == i) & (df[j] != k)]\n",
    "            item2.reset_index(inplace= True)\n",
    "            l = stats.ttest_ind(item1['rating'], item2['rating'])\n",
    "            #if l.statistic > 4.0:\n",
    "            #print(i), print(l)\n",
    "            if (l.statistic > cmax) :\n",
    "                cmax = l.statistic\n",
    "                con = k\n",
    "                context = j\n",
    "            #print(cmax)\n",
    "        \n",
    "    #print('finished')\n",
    "    if cmax > 2.5:\n",
    "        print('item ' + str(i) + ' cmax is ' + str(cmax) + ' con ' + str(con) + ' context ' + str(context))\n",
    "        some_list.append(MyClass(str(i),str(con),str(context)))\n",
    "\n",
    "      \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#should be the same path in Jupyter where this notebook exists\n",
    "notebook_path = os.path.abspath(\"CA_NCF_Movie_DePaulMovie.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+----+--------+---------+\n",
      "|UserID|   ItemID|Rating|Time|Location|Companion|\n",
      "+------+---------+------+----+--------+---------+\n",
      "|  1123|tt1499658|     2|  NA|      NA|       NA|\n",
      "|  1123|tt0405422|     4|  NA|      NA|       NA|\n",
      "|  1123|tt0109830|     5|  NA|      NA|       NA|\n",
      "|  1123|tt0088763|     3|  NA|      NA|       NA|\n",
      "|  1123|tt0133093|     3|  NA|      NA|       NA|\n",
      "+------+---------+------+----+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "UserID :   97\n",
      "ItemID :   79\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "#loading CSV data into a DataFrame\n",
    "\n",
    "Rating = Row(\"UserID\", \"ItemID\", \"Rating\",\"Time\",\"Location\",\"Companion\")\n",
    "\n",
    "ratingsget = os.path.join(os.path.dirname(notebook_path), \"datasets/DePaulMovie/ratings.csv\")\n",
    "ratings= sc.textFile(ratingsget) \\\n",
    "    .map(lambda line: line.split(\",\")[0:6])\\\n",
    "    .map(lambda line: (line[0], line[1], int(line[2]), line[3],line[4],line[5]))\\\n",
    "    .map(lambda r: Rating(*r))\n",
    "ratingDF_OLD_VERY = sqlContext.createDataFrame(ratings)\n",
    "ratingDF_OLD_VERY.show(5)\n",
    "\n",
    "someDFtt = ratingDF_OLD_VERY.select(col(\"UserID\")).distinct()\n",
    "\n",
    "print(\"UserID :   \" + str(someDFtt.count()))\n",
    "\n",
    "someDFtt1 = ratingDF_OLD_VERY.select(col(\"ItemID\")).distinct()\n",
    "\n",
    "print(\"ItemID :   \" + str(someDFtt1.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "####all the following cells are preparations for incorporating context into original items\n",
    "####we are actual constructing first new artificial items set\n",
    "####then we apply the NCF form the library as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "populateList = []\n",
    "def condition(i):\n",
    "    switcher={\n",
    "            'Home':[\"Home\", \"Cinema\",\"NA\"],\n",
    "            'Cinema':[\"Cinema\", \"Home\",\"NA\"],\n",
    "            'Weekday':[\"Weekday\", \"Weekend\", \"NA\"],\n",
    "            'Weekend':[\"Weekend\", \"Weekday\", \"NA\"],\n",
    "            'weekend':[\"weekend\", \"weekday\"],\n",
    "            'Partner':[\"Partner\", \"Family\", \"Alone\",\"NA\"],\n",
    "            'Family':[\"Family\", \"Partner\", \"Alone\",\"NA\"],\n",
    "            'Alone':[\"Alone\", \"Partner\", \"Family\",\"NA\"],\n",
    "           \n",
    "         }\n",
    "    return switcher.get(i,\"Invalid year\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, Binarizer\n",
    "from pyspark.sql.functions import rand \n",
    "\n",
    "#transformations\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"UserID\", outputCol=\"newUserID1\")\n",
    "indexer1 = StringIndexer(inputCol=\"ItemID\", outputCol=\"indexedItemID2\")\n",
    "indexer2 = StringIndexer(inputCol=\"artItemID1\", outputCol=\"artItemID2\")\n",
    "binarizer22 = Binarizer(threshold=2.0, inputCol=\"locationIndexed\", outputCol=\"binarized_location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[indexer1])\n",
    "                \n",
    "ratingDF_OLD = pipeline.fit(ratingDF_OLD_VERY).transform(ratingDF_OLD_VERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratingDF_OLD.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Home', 'Cinema', 'NA']\n",
      "Location\n",
      "+---------+------------------+-------+--------+---------+--------------+\n",
      "|   ItemID|binarized_location|   Time|Location|Companion|indexedItemID2|\n",
      "+---------+------------------+-------+--------+---------+--------------+\n",
      "|tt2251217|               1.0|     NA|      NA|       NA|          67.0|\n",
      "|tt2251217|               1.0|     NA|      NA|       NA|          67.0|\n",
      "|tt2251217|               0.0|Weekday|  Cinema|    Alone|          67.0|\n",
      "|tt2251217|               0.0|Weekday|  Cinema|   Family|          67.0|\n",
      "|tt2251217|               0.0|Weekday|  Cinema|  Partner|          67.0|\n",
      "+---------+------------------+-------+--------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "['Weekday', 'Weekend', 'NA']\n",
      "Time\n",
      "+---------+------------------+-------+--------+---------+--------------+\n",
      "|   ItemID|binarized_location|   Time|Location|Companion|indexedItemID2|\n",
      "+---------+------------------+-------+--------+---------+--------------+\n",
      "|tt3203616|               1.0|     NA|      NA|       NA|          76.0|\n",
      "|tt3203616|               0.0|Weekday|  Cinema|   Family|          76.0|\n",
      "|tt3203616|               0.0|Weekend|  Cinema|   Family|          76.0|\n",
      "|tt3203616|               0.0|Weekday|    Home|  Partner|          76.0|\n",
      "|tt3203616|               1.0|     NA|      NA|       NA|          76.0|\n",
      "+---------+------------------+-------+--------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, udf\n",
    "df_list = []\n",
    "toDiscard_df_list = []\n",
    "#constructing the dataFrame\n",
    "for obj in some_list:\n",
    "    itemid= obj.item\n",
    "    ctxCond = obj.condition\n",
    "    ctx = obj.context\n",
    "    partialDF = ratingDF_OLD.withColumn(\"newUserID1\",col(\"UserID\")).select(\"*\").where((col(\"ItemID\")==itemid) )\n",
    "    toDiscard_df_list.append(partialDF)\n",
    "    #print(partialDF.count())\n",
    "    populateList = condition(ctxCond)\n",
    "    print(populateList)\n",
    "    print(ctx)\n",
    "    general_udf = udf(lambda somename: categorical_from_vocab_list(somename, populateList, start=1))\n",
    "    partialDF_indexed = partialDF.withColumn(\"locationIndexed\", general_udf(col(ctx)).cast(\"double\"))\n",
    "    #print(partialDF_indexed.where(col('locationIndexed') != 0.0).count())\n",
    "    #print(partialDF_indexed.count())\n",
    "    pipeline1 = Pipeline(stages = [binarizer22])\n",
    "    allDF_temp =  pipeline1.fit(partialDF_indexed).transform(partialDF_indexed)\n",
    "    allDF_temp.select('ItemID','binarized_location','Time','Location','Companion','indexedItemID2').show(5)\n",
    "\n",
    "    bucket_cross_udf = udf(lambda feature1, feature2: hash_bucket(str(feature1) + \"_\" + str(feature2), bucket_size=10000))\n",
    "    df_list.append(allDF_temp.withColumn(\"artItemID1\", bucket_cross_udf(col(\"indexedItemID2\"), col(\"binarized_location\")).cast(\"int\") + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDF_prepare = reduce(DataFrame.unionAll, df_list).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allDF_prepare.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toDiscard_df_all_sub.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "toDiscard_df_all = reduce(DataFrame.unionAll, toDiscard_df_list).cache()\n",
    "#print(toDiscard_df_all.select(col('artItemID1')).distinct().count())\n",
    "toDiscard_df_all_sub = toDiscard_df_all.select(\"*\")\n",
    "ratingDF_OLD_sub = ratingDF_OLD.withColumn(\"newUserID1\",col(\"UserID\"))\n",
    "remainingDF = ratingDF_OLD_sub.subtract(toDiscard_df_all_sub)\n",
    "\n",
    "remainingDF_indexed = remainingDF.withColumn('binarized_location',lit(1.0))\n",
    "bucket_cross_udf1 = udf(lambda feature1, feature2: hash_bucket(str(feature1) + \"_\" + str(feature2), bucket_size=10000))\n",
    "remainingDF_indexed_all = remainingDF_indexed.withColumn(\"artItemID1\", bucket_cross_udf1(col(\"ItemID\"), col(\"binarized_location\")).cast(\"int\") + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+----+--------+---------+--------------+----------+\n",
      "|UserID|   ItemID|Rating|Time|Location|Companion|indexedItemID2|newUserID1|\n",
      "+------+---------+------+----+--------+---------+--------------+----------+\n",
      "|  1020|tt2251217|     2|  NA|      NA|       NA|          67.0|      1020|\n",
      "|  1074|tt2251217|     4|  NA|      NA|       NA|          67.0|      1074|\n",
      "+------+---------+------+----+--------+---------+--------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toDiscard_df_all_sub.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+----+--------+---------+--------------+----------+\n",
      "|UserID|   ItemID|Rating|Time|Location|Companion|indexedItemID2|newUserID1|\n",
      "+------+---------+------+----+--------+---------+--------------+----------+\n",
      "|  1123|tt1499658|     2|  NA|      NA|       NA|          38.0|      1123|\n",
      "|  1123|tt0405422|     4|  NA|      NA|       NA|          18.0|      1123|\n",
      "+------+---------+------+----+--------+---------+--------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratingDF_OLD_sub.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remainingDF_indexed_all_sub = remainingDF_indexed_all.select(col('newUserID1') ,col('indexedItemID2'),col(\"UserID\"),\n",
    "                                        col('Rating'),col('ItemID'),col('artItemID1'),col(\"binarized_location\"))\n",
    "allDF_prepare_sub_sub = allDF_prepare.select(col(\"newUserID1\") ,col(\"indexedItemID2\"),col(\"UserID\"),\n",
    "                                        col(\"Rating\"),col(\"ItemID\"),col(\"artItemID1\"),col(\"binarized_location\"))\n",
    "allDF_difinite = remainingDF_indexed_all_sub.union(allDF_prepare_sub_sub).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "someDF_art = allDF_difinite.select(col(\"artItemID1\")).distinct()\n",
    "print(someDF_art.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline(stages = [indexer2])\n",
    "\n",
    "allDF =  pipeline2.fit(allDF_difinite).transform(allDF_difinite)\n",
    "\n",
    "someDF1 = allDF.select((col(\"newUserID1\").cast(\"int\") ).alias(\"newUserID\"),col(\"indexedItemID2\").cast(\"int\"),\n",
    "                       col(\"Rating\"),col(\"ItemID\"),(col(\"artItemID2\").cast(\"int\") + 1).alias(\"aritid\"))\n",
    "someDF2 = allDF.select((col(\"newUserID1\").cast(\"int\")).alias(\"newUserID\"),\n",
    "                       (col(\"artItemID2\").cast(\"int\") + 1).alias(\"artItemID\"),col(\"Rating\"))#.distinct()\n",
    "\n",
    "someDF_OLD = someDF2.orderBy(rand())#.select(\"*\").limit(5000000)\n",
    "\n",
    "\n",
    "ratingDF_indexed1 = allDF.select((col(\"newUserID1\").cast(\"int\")).alias(\"usrID\"),col(\"UserID\"),\n",
    "                                 col(\"indexedItemID2\").cast(\"int\"),col(\"Rating\"),col(\"ItemID\"),\n",
    "                                 col(\"binarized_location\"), \n",
    "                                 (col(\"artItemID2\") .cast(\"int\") + 1).alias(\"aritid\"))\n",
    "\n",
    "ratingDF_indexed2 = allDF.select((col(\"newUserID1\").cast(\"int\")).alias(\"usrID\"),\n",
    "                                 (col(\"indexedItemID2\").cast(\"int\") + 1).alias(\"indexedItemID1\"),\n",
    "                                 col(\"Rating\"))#.distinct()\n",
    "\n",
    "CARSkit_DF = ratingDF_indexed1.select(col(\"usrID\").alias(\"userID\"),col(\"ItemID\"),  col(\"Rating\"),\n",
    "                                      col(\"binarized_location\"))\n",
    "CARSkit_DF_path = os.path.join(os.path.dirname(notebook_path), \"datasets/tripAdvisor/\")\n",
    "CARSkit='CARSkit-'+dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "CARSkit_DF.coalesce(1).write.csv(CARSkit_DF_path + CARSkit + '.csv')\n",
    "\n",
    "#shuffling data\n",
    "ratingDF_indexed_OLD = ratingDF_indexed2.orderBy(rand())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard items with less than 20 ratings (interactions)\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "w = Window.partitionBy('artItemID')\n",
    "someDF_NEW = someDF_OLD.withColumn('count_artItemID', f.count('newUserID').over(w))\\\n",
    "    .where(f.col('count_artItemID') > 5)\\\n",
    "    .drop('count_artItemID')#.select(\"*\").limit(5000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('newUserID')\n",
    "someDF = someDF_NEW.withColumn('count_newUserID', f.count('artItemID').over(w))\\\n",
    ".where(f.col('count_newUserID') > 5).drop('count_newUserID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#someDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TWEAKABLE: drop more items with less than a threshold to account for sparsity\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "w = Window.partitionBy('indexedItemID1')\n",
    "ratingDF_indexed = ratingDF_indexed_OLD.withColumn('count_ItemID', f.count('usrID').over(w))\\\n",
    "    .where(f.col('count_ItemID') > 5)\\\n",
    "    .drop('count_artItemID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratingDF_indexed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1122 80\n"
     ]
    }
   ],
   "source": [
    "someDFt3 = someDF.select(col(\"newUserID\")).distinct()\n",
    "max_user_id1 =  someDF.agg({\"newUserID\": \"max\"}).collect()[0][0]\n",
    "someDFt4 = someDF.select(col(\"artItemID\")).distinct()\n",
    "max_item_id1 = someDF.agg({\"artItemID\": \"max\"}).collect()[0][0]\n",
    "print(max_user_id1, max_item_id1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comprehensing your data. \n",
    "a tuple is (userid, movieid, rating). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usrID :   97\n",
      "1123 79\n"
     ]
    }
   ],
   "source": [
    "someDFt = ratingDF_indexed.select(col(\"usrID\")).distinct()\n",
    "\n",
    "print(\"usrID :   \" + str(someDFt.count()))\n",
    "\n",
    "max_user_id = ratingDF_indexed.agg({\"usrID\": \"max\"}).collect()[0][0]\n",
    "\n",
    "someDFt1 = ratingDF_indexed.select(col(\"indexedItemID1\")).distinct()\n",
    "\n",
    "max_item_id  = ratingDF_indexed.agg({\"indexedItemID1\": \"max\"}).collect()[0][0]\n",
    "print(max_user_id, max_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "####now that we have constructed artificial items set, we are then ready to apply NCF from BigDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming raw rarings into an RDD . \n",
    "BigDl 6.0 requires input as RDD of samples, BigDL data structure that can be formed by two numpy arrays\n",
    "containing features and lables for the first and second array, respectively.\n",
    "in the following format Sample.from_ndarray(feature, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<zoo.models.recommendation.recommender.UserItemFeature at 0x7f9d09a52588>,\n",
       " <zoo.models.recommendation.recommender.UserItemFeature at 0x7f9d09a528d0>,\n",
       " <zoo.models.recommendation.recommender.UserItemFeature at 0x7f9d09a4f400>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def build_sample(userid, itemid, rating):\n",
    "    sampleTaken = Sample.from_ndarray(np.array([userid, itemid]), np.array([rating]))\n",
    "    return UserItemFeature(userid, itemid, sampleTaken)\n",
    "#CA-NCF settings\n",
    "pairFeatureRdds_CANCF = someDF.rdd.map(lambda x: build_sample(x[0], x[1],x[2]))\n",
    "pairFeatureRdds_CANCF.repartition(4)\n",
    "\n",
    "\n",
    "pairFeatureRdds_CANCF.take(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80% training data, 20% testing, randomly split\n",
    "\n",
    "import time\n",
    "millis = int(round(time.time() * 1000))\n",
    "\n",
    "#CANCF settings\n",
    "#tweakable, seed= 42L\n",
    "trainPairFeatureRdds_CANCF, valPairFeatureRdds_CANCF = pairFeatureRdds_CANCF.randomSplit([0.8, 0.2], seed = millis)\n",
    "valPairFeatureRdds_CANCF.cache()\n",
    "train_rdd_CANCF= trainPairFeatureRdds_CANCF.map(lambda pair_feature: pair_feature.sample)\n",
    "val_rdd_CANCF= valPairFeatureRdds_CANCF.map(lambda pair_feature: pair_feature.sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applying Nerual Collaborative filtering (He, 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooNeuralCF\n"
     ]
    }
   ],
   "source": [
    "#Neural Collaborative Filtering (NCF) model in BigDL 6.0\n",
    "#see more parameters here:\n",
    "#https://github.com/intel-analytics/analytics-zoo/blob/master/docs/docs/APIGuide/Models/recommendation.md\n",
    "\n",
    "#more hyperparameters  ,user_embed=40, item_embed=2\n",
    "ncf = NeuralCF(user_count=max_user_id1, item_count=max_item_id1, class_num=5, hidden_layers=[20,10])#, include_mf = True,mf_embed = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserItemPrediction [user_id: 1025, item_id: 44, prediction: 4, probability: 0.23324161922705944]\n",
      "UserItemPrediction [user_id: 1025, item_id: 10, prediction: 4, probability: 0.23419799421693552]\n",
      "UserItemPrediction [user_id: 1025, item_id: 14, prediction: 4, probability: 0.23587748289700058]\n",
      "UserItemPrediction [user_id: 1084, item_id: 1, prediction: 4, probability: 0.2362773049811907]\n",
      "UserItemPrediction [user_id: 1084, item_id: 1, prediction: 4, probability: 0.2362773049811907]\n"
     ]
    }
   ],
   "source": [
    "##predicting user-item pairs using CA-NCF\n",
    "predictUserItem_CANCF = ncf.predict_user_item_pair(valPairFeatureRdds_CANCF)\n",
    "for ans in predictUserItem_CANCF.take(5): print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserItemPrediction [user_id: 1001, item_id: 19, prediction: 4, probability: 0.23597276858863753]\n",
      "UserItemPrediction [user_id: 1001, item_id: 19, prediction: 4, probability: 0.23597276858863753]\n",
      "UserItemPrediction [user_id: 1001, item_id: 11, prediction: 4, probability: 0.2355444824288736]\n",
      "UserItemPrediction [user_id: 1001, item_id: 7, prediction: 4, probability: 0.23535668014321895]\n",
      "UserItemPrediction [user_id: 1001, item_id: 5, prediction: 4, probability: 0.2352673925995903]\n",
      "UserItemPrediction [user_id: 1002, item_id: 7, prediction: 4, probability: 0.23542826367545852]\n",
      "UserItemPrediction [user_id: 1002, item_id: 66, prediction: 4, probability: 0.2348852112055863]\n",
      "UserItemPrediction [user_id: 1002, item_id: 43, prediction: 4, probability: 0.23479739006154166]\n",
      "UserItemPrediction [user_id: 1002, item_id: 43, prediction: 4, probability: 0.23479739006154166]\n",
      "UserItemPrediction [user_id: 1002, item_id: 1, prediction: 4, probability: 0.23424573984901648]\n"
     ]
    }
   ],
   "source": [
    "#Recommending 5 items for every user\n",
    "recItemsForUsers_CANCF = ncf.recommend_for_user(valPairFeatureRdds_CANCF, 5)\n",
    "for ans in recItemsForUsers_CANCF.take(10): print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
