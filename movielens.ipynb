{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NCF Recommender with Explict Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#Context-Aware Neural Collaborative Filtering (CA-NCF)\n",
    "#Author: Isam Mashhour Al Jawarneh\n",
    "#Date: 30 January 2019\n",
    "from zoo.models.recommendation import *\n",
    "from zoo.models.recommendation.utils import *\n",
    "from zoo.common.nncontext import init_nncontext\n",
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.dataset.base import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.optim import *\n",
    "from bigdl.util.common import *\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = init_nncontext(create_spark_conf().setAppName(\"CA_NCF Movielens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import researchpy as rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/movies/movielens1.csv\", names=['UserID', 'ItemID', 'rating','dayOfWeek', 'month','year'], header=None, usecols=[0,1,2,3,9,10])[['UserID', 'ItemID', 'rating','dayOfWeek','month' ,'year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for constructing objects containing items, the contexts and context conditions\n",
    "\n",
    "class MyClass(object):\n",
    "    def __init__(self, item,condition,context):\n",
    "        self.number = number\n",
    "        self.item = item\n",
    "        self.condition = condition\n",
    "        self.context = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some_list = []\n",
    "#some_list.append(MyClass('2','weekend','dayOfWeek'))\n",
    "#some_list.append(MyClass('3', 'M', 'gender'))\n",
    "#for obj in some_list:\n",
    " #   print(obj.item)\n",
    "  #  print(obj.condition)\n",
    "   # print(obj.context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######ITEM SPLITTING Procedure############\n",
    "## as described in :\n",
    "## Experimental evaluation of context-dependent collaborative filtering using item splitting\n",
    "## https://dl.acm.org/citation.cfm?id=2579997\n",
    "#df = df1.head(1000)\n",
    "## output a list (item, condition, context), where condition is the context condition that will be used for splitting the item\n",
    "\n",
    "#profiling data\n",
    "some_list = []\n",
    "contextFactors = ['year','dayOfWeek']\n",
    "dflist = df['ItemID'].unique().tolist()\n",
    "#array_length = len(contextFactors)\n",
    "#print(array_length)\n",
    "#j = 1\n",
    "cmax = 0.0\n",
    "con = ''\n",
    "context = ''\n",
    "for i in dflist:\n",
    "    cmax = 0.0\n",
    "    for j in contextFactors:\n",
    "        #print(j)\n",
    "        conditionList = df[j].unique().tolist()\n",
    "        #array_length1 = len(conditionList)\n",
    "        #print(array_length1)\n",
    "        for k in conditionList:\n",
    "            #print(k)\n",
    "            item1 = df[(df['ItemID'] == i) & (df[j] == k)]\n",
    "            item1.reset_index(inplace= True)  \n",
    "            item2 = df[(df['ItemID'] == i) & (df[j] != k)]\n",
    "            item2.reset_index(inplace= True)\n",
    "            l = stats.ttest_ind(item1['rating'], item2['rating'])\n",
    "            #if l.statistic > 4.0:\n",
    "            #print(i), print(l)\n",
    "            if (l.statistic > cmax) :\n",
    "                cmax = l.statistic\n",
    "                con = k\n",
    "                context = j\n",
    "            #print(cmax)\n",
    "        \n",
    "    #print('finished')\n",
    "    if cmax > 4.0:\n",
    "        print('item ' + str(i) + ' cmax is ' + str(cmax) + ' con ' + str(con) + ' context ' + str(context))\n",
    "        some_list.append(MyClass(str(i),str(con),str(context)))\n",
    "\n",
    "      \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this acts as a pre-trained model\n",
    "# this is a partial list only of the items to split. known by profiling the data\n",
    "# for a full list of items to split: disable this cell and enable the previous cell\n",
    "some_list = []\n",
    "some_list.append(MyClass('3578','2000','year'))\n",
    "some_list.append(MyClass('3793','2000','year'))\n",
    "some_list.append(MyClass('3948','2000','year'))\n",
    "some_list.append(MyClass('3751','2000','year'))\n",
    "some_list.append(MyClass('106','weekday','dayOfWeek'))\n",
    "some_list.append(MyClass('815','weekend','dayOfWeek'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#should be the same path in Jupyter where this notebook exists\n",
    "notebook_path = os.path.abspath(\"movielens.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---------+------+---+----------+----------+---------+-----+----+\n",
      "|UserID|ItemID|rating|dayOfWeek|gender|age|occupation|dayOfMonth|dayOfYear|month|year|\n",
      "+------+------+------+---------+------+---+----------+----------+---------+-----+----+\n",
      "|     1|  1193|     5|  weekend|     F|  1|        10|        31|      366|   11|2000|\n",
      "|     1|   661|     3|  weekend|     F|  1|        10|        31|      366|   11|2000|\n",
      "|     1|   914|     3|  weekend|     F|  1|        10|        31|      366|   11|2000|\n",
      "|     1|  3408|     4|  weekend|     F|  1|        10|        31|      366|   11|2000|\n",
      "|     1|  2355|     5|  weekend|     F|  1|        10|         7|        7|    0|2001|\n",
      "+------+------+------+---------+------+---+----------+----------+---------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "#CHANGE rating to Rating for 1million dataset\n",
    "Rating = Row(\"UserID\", \"ItemID\", \"rating\",\"dayOfWeek\",\"gender\",\"age\",\"occupation\",\"dayOfMonth\",\"dayOfYear\",\"month\",\"year\")\n",
    "#UserID,ItemID,Rating,UserState,UserTimeZone,ItemCity,ItemState,ItemTimeZone,TripType\n",
    "\n",
    "ratingsget = os.path.join(os.path.dirname(notebook_path), \"datasets/movies/movielens1.csv\")\n",
    "ratings= sc.textFile(ratingsget) \\\n",
    "    .map(lambda line: line.split(\",\")[0:11])\\\n",
    "    .map(lambda line: (line[0], line[1], int(line[2]), line[3],line[4],line[5], line[6],line[7],line[8],line[9],line[10]))\\\n",
    "    .map(lambda r: Rating(*r))\n",
    "ratingDF_OLD = sqlContext.createDataFrame(ratings)\n",
    "ratingDF_OLD.show(5)\n",
    "\n",
    "someDFtt = ratingDF_OLD.select(col(\"UserID\")).distinct()\n",
    "\n",
    "#max_movie_id_alt = someDFt.count()\n",
    "\n",
    "#print(\"UserID :   \" + str(someDFtt.count()))\n",
    "\n",
    "someDFtt1 = ratingDF_OLD.select(col(\"ItemID\")).distinct()\n",
    "\n",
    "#max_movie_id_alt = someDFt.count()\n",
    "\n",
    "#print(\"ItemID :   \" + str(someDFtt1.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context = 'dayOfWeek'\n",
    "populateList = []\n",
    "#if context == 'year':\n",
    "def condition(i):\n",
    "    switcher={\n",
    "            '2000':[\"2000\", \"2001\",\"2002\",\"2003\"],\n",
    "            '2001':[\"2001\", \"2000\",\"2002\",\"2003\"],\n",
    "            '2002':[\"2002\", \"2001\",\"2000\",\"2003\"],\n",
    "            '2003':[\"2003\", \"2001\",\"2002\",\"2000\"],\n",
    "            'weekend':[\"weekend\", \"weekday\"],\n",
    "            'weekday':[\"weekday\", \"weekend\"],\n",
    "            'M':[\"M\",\"F\"],\n",
    "            'F':[\"F\",\"M\"],\n",
    "         }\n",
    "    return switcher.get(i,\"Invalid year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, Binarizer\n",
    "from pyspark.sql.functions import rand \n",
    "\n",
    "indexer = StringIndexer(inputCol=\"UserID\", outputCol=\"newUserID1\")\n",
    "indexer1 = StringIndexer(inputCol=\"ItemID\", outputCol=\"indexedItemID2\")\n",
    "indexer2 = StringIndexer(inputCol=\"artItemID1\", outputCol=\"artItemID2\")\n",
    "binarizer22 = Binarizer(threshold=1.0, inputCol=\"locationIndexed\", outputCol=\"binarized_location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2000', '2001', '2002', '2003']\n",
      "year\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "|ItemID|binarized_location|gender|dayOfWeek|year|indexedItemID2|\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "|  3578|               0.0|     M|  weekend|2000|          3578|\n",
      "|  3578|               0.0|     M|  weekend|2000|          3578|\n",
      "|  3578|               0.0|     F|  weekend|2000|          3578|\n",
      "|  3578|               0.0|     M|  weekend|2000|          3578|\n",
      "|  3578|               0.0|     M|  weekend|2000|          3578|\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "['2000', '2001', '2002', '2003']\n",
      "year\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "|ItemID|binarized_location|gender|dayOfWeek|year|indexedItemID2|\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "|  3793|               0.0|     M|  weekend|2000|          3793|\n",
      "|  3793|               0.0|     M|  weekend|2000|          3793|\n",
      "|  3793|               0.0|     M|  weekend|2000|          3793|\n",
      "|  3793|               0.0|     M|  weekend|2000|          3793|\n",
      "|  3793|               0.0|     F|  weekend|2000|          3793|\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "['2000', '2001', '2002', '2003']\n",
      "year\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "|ItemID|binarized_location|gender|dayOfWeek|year|indexedItemID2|\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "|  3948|               0.0|     M|  weekend|2000|          3948|\n",
      "|  3948|               0.0|     F|  weekend|2000|          3948|\n",
      "|  3948|               0.0|     M|  weekend|2000|          3948|\n",
      "|  3948|               1.0|     M|  weekend|2001|          3948|\n",
      "|  3948|               0.0|     M|  weekday|2000|          3948|\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "['2000', '2001', '2002', '2003']\n",
      "year\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "|ItemID|binarized_location|gender|dayOfWeek|year|indexedItemID2|\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "|  3751|               0.0|     M|  weekend|2000|          3751|\n",
      "|  3751|               0.0|     F|  weekend|2000|          3751|\n",
      "|  3751|               0.0|     M|  weekend|2000|          3751|\n",
      "|  3751|               0.0|     F|  weekend|2000|          3751|\n",
      "|  3751|               1.0|     M|  weekday|2001|          3751|\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "['weekday', 'weekend']\n",
      "dayOfWeek\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "|ItemID|binarized_location|gender|dayOfWeek|year|indexedItemID2|\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "|   106|               0.0|     M|  weekday|2000|           106|\n",
      "|   106|               0.0|     F|  weekday|2000|           106|\n",
      "|   106|               0.0|     M|  weekday|2000|           106|\n",
      "|   106|               0.0|     F|  weekday|2000|           106|\n",
      "|   106|               0.0|     M|  weekday|2000|           106|\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "['weekend', 'weekday']\n",
      "dayOfWeek\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "|ItemID|binarized_location|gender|dayOfWeek|year|indexedItemID2|\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "|   815|               1.0|     M|  weekday|2000|           815|\n",
      "|   815|               0.0|     M|  weekend|2000|           815|\n",
      "|   815|               1.0|     M|  weekday|2000|           815|\n",
      "|   815|               1.0|     M|  weekday|2000|           815|\n",
      "|   815|               0.0|     M|  weekend|2000|           815|\n",
      "+------+------------------+------+---------+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, udf\n",
    "df_list = []\n",
    "toDiscard_df_list = []\n",
    "\n",
    "for obj in some_list:\n",
    "    itemid= obj.item\n",
    "    ctxCond = obj.condition\n",
    "    ctx = obj.context\n",
    "    partialDF = ratingDF_OLD.withColumn(\"newUserID1\",col(\"UserID\")).withColumn(\"indexedItemID2\",col(\"ItemID\")).select(\"*\").where((col(\"ItemID\")==itemid) )\n",
    "    toDiscard_df_list.append(partialDF)\n",
    "\n",
    "    populateList = condition(ctxCond)\n",
    "    print(populateList)\n",
    "    print(ctx)\n",
    "    general_udf = udf(lambda somename: categorical_from_vocab_list(somename, populateList, start=1))\n",
    "    partialDF_indexed = partialDF.withColumn(\"locationIndexed\", general_udf(col(ctx)).cast(\"double\"))\n",
    "\n",
    "    pipeline1 = Pipeline(stages = [binarizer22])\n",
    "    allDF_temp =  pipeline1.fit(partialDF_indexed).transform(partialDF_indexed)\n",
    "    allDF_temp.select('ItemID','binarized_location','gender','dayOfWeek','year','indexedItemID2').show(5)\n",
    "\n",
    "    bucket_cross_udf = udf(lambda feature1, feature2: hash_bucket(str(feature1) + \"_\" + str(feature2), bucket_size=10000))\n",
    "    df_list.append(allDF_temp.withColumn(\"artItemID1\", bucket_cross_udf(col(\"indexedItemID2\"), col(\"binarized_location\")).cast(\"int\") + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDF_prepare = reduce(DataFrame.unionAll, df_list).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---------+------+---+----------+----------+---------+-----+----+----------+--------------+---------------+------------------+----------+\n",
      "|UserID|ItemID|rating|dayOfWeek|gender|age|occupation|dayOfMonth|dayOfYear|month|year|newUserID1|indexedItemID2|locationIndexed|binarized_location|artItemID1|\n",
      "+------+------+------+---------+------+---+----------+----------+---------+-----+----+----------+--------------+---------------+------------------+----------+\n",
      "|     2|  3578|     5|  weekend|     M| 56|        16|        31|      366|   11|2000|         2|          3578|            1.0|               0.0|      1049|\n",
      "+------+------+------+---------+------+---+----------+----------+---------+-----+----+----------+--------------+---------------+------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "allDF_prepare.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "toDiscard_df_all = reduce(DataFrame.unionAll, toDiscard_df_list).cache()\n",
    "#print(toDiscard_df_all.select(col('artItemID1')).distinct().count())\n",
    "toDiscard_df_all_sub = toDiscard_df_all.select(\"*\")\n",
    "ratingDF_OLD_sub = ratingDF_OLD.withColumn(\"newUserID1\",col(\"UserID\")).withColumn(\"indexedItemID2\",col(\"ItemID\"))\n",
    "remainingDF = ratingDF_OLD_sub.subtract(toDiscard_df_all_sub)\n",
    "\n",
    "remainingDF_indexed = remainingDF.withColumn('binarized_location',lit(1.0))\n",
    "bucket_cross_udf1 = udf(lambda feature1, feature2: hash_bucket(str(feature1) + \"_\" + str(feature2), bucket_size=1200000))\n",
    "remainingDF_indexed_all = remainingDF_indexed.withColumn(\"artItemID1\", bucket_cross_udf1(col(\"ItemID\"), col(\"binarized_location\")).cast(\"int\") + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---------+------+---+----------+----------+---------+-----+----+----------+--------------+------------------+----------+\n",
      "|UserID|ItemID|rating|dayOfWeek|gender|age|occupation|dayOfMonth|dayOfYear|month|year|newUserID1|indexedItemID2|binarized_location|artItemID1|\n",
      "+------+------+------+---------+------+---+----------+----------+---------+-----+----+----------+--------------+------------------+----------+\n",
      "|     1|  1836|     5|  weekend|     F|  1|        10|        31|      366|   11|2000|         1|          1836|               1.0|   1146344|\n",
      "+------+------+------+---------+------+---+----------+----------+---------+-----+----+----------+--------------+------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remainingDF_indexed_all.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remainingDF_indexed_all_sub = remainingDF_indexed_all.select(col('newUserID1') ,col('indexedItemID2'),col(\"UserID\"),\n",
    "                                        col('Rating'),col('ItemID'),col('artItemID1'),col(\"binarized_location\"))\n",
    "allDF_prepare_sub_sub = allDF_prepare.select(col(\"newUserID1\") ,col(\"indexedItemID2\"),col(\"UserID\"),\n",
    "                                        col(\"Rating\"),col(\"ItemID\"),col(\"artItemID1\"),col(\"binarized_location\"))\n",
    "allDF_difinite = remainingDF_indexed_all_sub.union(allDF_prepare_sub_sub).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3706\n"
     ]
    }
   ],
   "source": [
    "someDF_art = allDF_difinite.select(col(\"artItemID1\")).distinct()\n",
    "print(someDF_art.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline(stages = [indexer2])\n",
    "\n",
    "allDF =  pipeline2.fit(allDF_difinite).transform(allDF_difinite)\n",
    "\n",
    "someDF1 = allDF.select((col(\"newUserID1\").cast(\"int\") ).alias(\"newUserID\"),col(\"indexedItemID2\").cast(\"int\"),\n",
    "                       col(\"Rating\"),col(\"ItemID\"),(col(\"artItemID2\").cast(\"int\") + 1).alias(\"aritid\"))\n",
    "someDF2 = allDF.select((col(\"newUserID1\").cast(\"int\")).alias(\"newUserID\"),(col(\"artItemID2\").cast(\"int\") + 1).alias(\"artItemID\"),col(\"Rating\"))#.distinct()\n",
    "\n",
    "someDF_OLD = someDF2.orderBy(rand())#.select(\"*\").limit(5000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE discard items with less than 20 ratings (interactions)\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "w = Window.partitionBy('artItemID')\n",
    "someDF_NEW = someDF_OLD.withColumn('count_artItemID', f.count('newUserID').over(w))\\\n",
    "    .where(f.col('count_artItemID') > 100)\\\n",
    "    .drop('count_artItemID')#.select(\"*\").limit(5000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('newUserID')\n",
    "someDF = someDF_NEW.withColumn('count_newUserID', f.count('artItemID').over(w))\\\n",
    "    .where(f.col('count_newUserID') > 20)\\\n",
    "    .drop('count_newUserID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 2010\n"
     ]
    }
   ],
   "source": [
    "someDFt3 = someDF.select(col(\"newUserID\")).distinct()\n",
    "\n",
    "max_user_id1 =  someDF.agg({\"newUserID\": \"max\"}).collect()[0][0]\n",
    "\n",
    "someDFt4 = someDF.select(col(\"artItemID\")).distinct()\n",
    "\n",
    "max_item_id1 = someDF.agg({\"artItemID\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(max_user_id1, max_item_id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<zoo.models.recommendation.recommender.UserItemFeature at 0x7ff53405c668>,\n",
       " <zoo.models.recommendation.recommender.UserItemFeature at 0x7ff53405c1d0>,\n",
       " <zoo.models.recommendation.recommender.UserItemFeature at 0x7ff531977e10>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_sample(user_id, item_id, rating):\n",
    "    sample = Sample.from_ndarray(np.array([user_id, item_id]), np.array([rating]))\n",
    "    return UserItemFeature(user_id, item_id, sample)\n",
    "#CA-NCF settings\n",
    "pairFeatureRdds_CANCF = someDF.rdd.map(lambda x: build_sample(x[0], x[1],x[2]))\n",
    "#pairFeatureRdds = someDF.rdd.map(lambda x: build_sample(x[0], x[1],x[2]))\n",
    "\n",
    "#CHANGE repartition1\n",
    "pairFeatureRdds_CANCF.repartition(1)\n",
    "pairFeatureRdds_CANCF.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooNeuralCF\n"
     ]
    }
   ],
   "source": [
    "#see more parameters here:\n",
    "#https://github.com/intel-analytics/analytics-zoo/blob/master/docs/docs/APIGuide/Models/recommendation.md\n",
    "#CHANGE ADD user_embed=20, item_embed=20, \n",
    "ncf = NeuralCF(user_count=max_user_id1, item_count=max_item_id1, class_num=5, hidden_layers=[20,10],include_mf = False)#, include_mf = True,mf_embed = 10)  user_embed=20, item_embed=20, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "millis = int(round(time.time() * 1000))\n",
    "#CA-NCF settings\n",
    "trainPairFeatureRdds_CANCF, valPairFeatureRdds_CANCF = pairFeatureRdds_CANCF.randomSplit([0.8, 0.2], seed = millis)\n",
    "valPairFeatureRdds_CANCF.cache()\n",
    "train_rdd_CANCF= trainPairFeatureRdds_CANCF.map(lambda pair_feature: pair_feature.sample)\n",
    "val_rdd_CANCF= valPairFeatureRdds_CANCF.map(lambda pair_feature: pair_feature.sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserItemPrediction [user_id: 148, item_id: 1084, prediction: 4, probability: 0.27006351024788056]\n",
      "UserItemPrediction [user_id: 148, item_id: 853, prediction: 4, probability: 0.27096964846008004]\n",
      "UserItemPrediction [user_id: 148, item_id: 756, prediction: 4, probability: 0.2715697420252746]\n",
      "UserItemPrediction [user_id: 148, item_id: 606, prediction: 4, probability: 0.2712912773787427]\n",
      "UserItemPrediction [user_id: 148, item_id: 1210, prediction: 4, probability: 0.2708641380489278]\n"
     ]
    }
   ],
   "source": [
    "##predicting user-item pairs using CA-NCF\n",
    "predictUserItem_CANCF = ncf.predict_user_item_pair(valPairFeatureRdds_CANCF)\n",
    "for ans in predictUserItem_CANCF.take(5): print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserItemPrediction [user_id: 1200, item_id: 82, prediction: 4, probability: 0.2727277485333273]\n",
      "UserItemPrediction [user_id: 1200, item_id: 23, prediction: 4, probability: 0.27139912183679626]\n",
      "UserItemPrediction [user_id: 1200, item_id: 14, prediction: 4, probability: 0.2707353980416912]\n",
      "UserItemPrediction [user_id: 1200, item_id: 106, prediction: 4, probability: 0.2707231664058888]\n",
      "UserItemPrediction [user_id: 1200, item_id: 84, prediction: 4, probability: 0.2707103544406774]\n",
      "UserItemPrediction [user_id: 2200, item_id: 432, prediction: 4, probability: 0.2723625294827841]\n",
      "UserItemPrediction [user_id: 2200, item_id: 22, prediction: 4, probability: 0.27105952780326453]\n",
      "UserItemPrediction [user_id: 2200, item_id: 630, prediction: 4, probability: 0.2709920024347923]\n",
      "UserItemPrediction [user_id: 2200, item_id: 100, prediction: 4, probability: 0.2708530952576944]\n",
      "UserItemPrediction [user_id: 2200, item_id: 579, prediction: 4, probability: 0.2705317923256243]\n"
     ]
    }
   ],
   "source": [
    "#Recommending 5 items for every user\n",
    "recItemsForUsers_CANCF = ncf.recommend_for_user(valPairFeatureRdds_CANCF, 5)\n",
    "for ans in recItemsForUsers_CANCF.take(10): print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
